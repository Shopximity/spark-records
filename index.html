<html><head><meta charset="utf-8" /><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /><title>Spark Records</title><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="bulletproof Spark jobs" /><meta name="author" content="Swoop" /><meta name="og:image" content="/spark-records/img/poster.png" /><meta name="og:title" content="Spark Records" /><meta name="og:site_name" content="Spark Records" /><meta name="og:url" content="http://www.swoop.com" /><meta name="og:type" content="website" /><meta name="og:description" content="bulletproof Spark jobs" /><meta name="twitter:image" content="/spark-records/img/poster.png" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:site" content="" /><link rel="icon" type="image/png" href="/spark-records/img/favicon.png" /><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" /><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" /><link rel="stylesheet" href="/spark-records/highlight/styles/tomorrow.css" /><link rel="stylesheet" href="/spark-records/css/style.css" /><link rel="stylesheet" href="/spark-records/css/palette.css" /><link rel="stylesheet" href="/spark-records/css/overrides.css" /></head><body><header id="site-header"><div class="navbar-wrapper"><div class="container"><div class="row"><div class="col-xs-6"><a href="/spark-records/" class="brand"><div class="icon-wrapper"><span>Spark Records</span></div></a></div><div class="col-xs-6"><nav class="text-right"><ul class=""><li><a href="https://github.com/Shopximity/spark-records"><i class="fa fa-github"></i><span class="hidden-xs">GitHub</span></a></li><li><a href="/spark-records/docs.html"><i class="fa fa-file-text"></i><span class="hidden-xs">Documentation</span></a></li></ul></nav></div></div></div></div><div class="jumbotron"><div class="container"><h1 class="text-center">bulletproof Spark jobs</h1><h2></h2><p class="text-center"><a href="https://github.com/Shopximity/spark-records" class="btn btn-outline-inverse">View on GitHub</a></p></div></div><div><ul class="horizontalNav">        </ul></div></header><main id="site-main"><section class="use"><div class="container"><div id="content"><h2 id="spark-records">Spark Records</h2>

<p>Spark Records is a data processing pattern with an associated lightweight, dependency-free framework for <a href="https://spark.apache.org/" title="Apache Spark">Apache Spark v2+</a> that enables:</p>

<ol>
  <li>
    <p><strong>Bulletproof data processing with Spark</strong><br />
 Your jobs will never unpredictably fail midway due to data transformation bugs. Spark records give you predictable failure control through instant data quality checks performed on metrics automatically collected during job execution, without any additional querying.</p>
  </li>
  <li>
    <p><strong>Automatic row-level structured logging</strong><br />
 Exceptions generated during job execution are automatically associated with the data that caused the exception, down to nested exception causes and full stack traces. If you need to reprocess data, you can trivially and efficiently choose to only process the failed inputs.</p>
  </li>
  <li>
    <p><strong>Lightning-fast root cause analysis</strong><br />
 Get answers to any questions related to exceptions or warnings generated during job execution directly using SparkSQL or your favorite Spark DSL. Would you like to see the top 5 issues encountered during job execution with example source data and the line in your code that caused the problem? <a href="docs.html#root-cause-analysis">You can</a>.</p>
  </li>
</ol>

<p>Spark Records has been tested with petabyte-scale data at <a href="https://www.swoop.com">Swoop</a>. The library was extracted out of Swoop’s production systems to share with the Spark community.</p>

<p>See the <a href="docs.html" title="Spark Records Documentation">documentation</a> for more information or watch the <a href="https://spark-summit.org/east-2017/events/bulletproof-jobs-patterns-for-large-scale-spark-processing/">Spark Summit talk</a>.</p>

<div style="text-align: center;"><iframe src="//www.slideshare.net/slideshow/embed_code/key/TSOLI6UGKLFZE" width="595" height="374" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""> </iframe></div>

<h2 id="who-are-spark-records-for">Who are Spark Records for?</h2>

<p>Spark Records are for busy data engineers and data scientists who have to deal with complex data munging and/or unreliable/dirty data.</p>

<p>Spark Records are a data pattern. You can use it from <a href="docs.html#advanced-root-cause-analysis">any programming language</a>. Further, if your schema follows the pattern, the root cause analysis tooling in this framework, which is built in Scala, can be applied to your data even if the data has been produced using a different language. In other words, you don’t have to use Scala to take advantage of much of the value that Spark Records bring.</p>

<h2 id="overview">Overview</h2>

<p>Records provide a structured envelope around your data. The contract between the envelope and the framework code is what enables the magic of Spark Records. Defining records is easy:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.swoop.spark.records._</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">MyData</span><span class="o">(</span><span class="cm">/* whatever your data needs */</span><span class="o">)</span>

<span class="k">case</span> <span class="k">class</span> <span class="nc">MyDataRecord</span><span class="o">(</span>
  <span class="n">features</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span>                     <span class="c1">// features enable fast categorization</span>
  <span class="n">data</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">MyData</span><span class="o">]</span> <span class="k">=</span> <span class="nc">None</span><span class="o">,</span>       <span class="c1">// this is your data, whatever you need</span>
  <span class="n">source</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">MyInput</span><span class="o">]</span> <span class="k">=</span> <span class="nc">None</span><span class="o">,</span>    <span class="c1">// the source enables data provenance tracking</span>
  <span class="n">flight</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">None</span><span class="o">,</span>     <span class="c1">// related jobs are part of the same flight</span>
  <span class="n">issues</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">Issue</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">None</span>  <span class="c1">// this is the "row-level log"</span>
<span class="o">)</span> <span class="k">extends</span> <span class="nc">Record</span><span class="o">[</span><span class="kt">MyData</span>, <span class="kt">MyInput</span><span class="o">]</span>
</code></pre></div></div>

<p>The envelope can be extended to include other fields. Because most of the envelope values, other than <code class="language-plaintext highlighter-rouge">data</code>, are either the same or <code class="language-plaintext highlighter-rouge">null</code>, the envelope has almost no storage overhead (because of <a href="https://en.wikipedia.org/wiki/Run-length_encoding">run-length encoding</a> in data storage formats such as <a href="https://github.com/Parquet/parquet-format/blob/master/Encodings.md">Parquet</a>). In columnar storage formats, the envelope also has essentially no query overhead.</p>

<p>The idea behind Spark Records is that users of records don’t even know the records are there because you can expose just the data to them using views in SparkSQL or directly, in a manner that’s independent of how the data is stored (flat or not, partitioned or not, etc.).</p>

<p>Building records involves implementing three methods: one to create your data, one to wrap it in a record and one to create an error record. The last two are usually one liners, as shown below:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">case</span> <span class="k">class</span> <span class="nc">Builder</span><span class="o">(</span><span class="n">input</span><span class="k">:</span> <span class="kt">MyInput</span><span class="o">,</span> <span class="k">override</span> <span class="k">val</span> <span class="nv">jc</span><span class="k">:</span> <span class="kt">JobContext</span><span class="o">)</span>
  <span class="k">extends</span> <span class="nc">RecordBuilder</span><span class="o">[</span><span class="kt">MyInput</span>, <span class="kt">MyData</span>, <span class="kt">MyDataRecord</span>, <span class="kt">JobContext</span><span class="o">](</span><span class="n">input</span><span class="o">,</span> <span class="n">jc</span><span class="o">)</span> <span class="o">{</span>
	
  <span class="k">def</span> <span class="nf">buildData</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">MyData</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span> <span class="cm">/* your business logic here */</span> <span class="o">}</span>
	
  <span class="k">def</span> <span class="nf">dataRecord</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">MyData</span><span class="o">,</span> <span class="n">issues</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Issue</span><span class="o">])</span><span class="k">:</span> <span class="kt">MyDataRecord</span> <span class="o">=</span>
    <span class="nc">MyDataRecord</span><span class="o">(</span><span class="n">features</span><span class="o">,</span> <span class="nc">Some</span><span class="o">(</span><span class="n">data</span><span class="o">),</span> <span class="nc">None</span><span class="o">,</span> <span class="nv">jc</span><span class="o">.</span><span class="py">flight</span><span class="o">,</span> <span class="n">issues</span><span class="o">)</span>
	
  <span class="k">def</span> <span class="nf">errorRecord</span><span class="o">(</span><span class="n">issues</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Issue</span><span class="o">])</span><span class="k">:</span> <span class="kt">MyDataRecord</span> <span class="o">=</span>
    <span class="nc">MyDataRecord</span><span class="o">(</span><span class="n">features</span><span class="o">,</span> <span class="n">maybeData</span><span class="o">,</span> <span class="nc">Some</span><span class="o">(</span><span class="nv">input</span><span class="o">.</span><span class="py">toString</span><span class="o">),</span> <span class="nv">jc</span><span class="o">.</span><span class="py">flight</span><span class="o">,</span> <span class="n">issues</span><span class="o">)</span>
	
<span class="o">}</span>
</code></pre></div></div>

<p>That’s it. This is all you have to do to get most of the benefits of Spark Records: bulletproof exception management, automatic metrics collection and automatic data quality checks.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">jobContext</span><span class="o">.</span><span class="py">checkDataQuality</span><span class="o">(</span>
  <span class="n">minInputs</span>     <span class="k">=</span> <span class="mi">1000000</span><span class="o">,</span>   <span class="c1">// fail if fewer than 1 million inputs</span>
  <span class="n">maxErrorRate</span>  <span class="k">=</span> <span class="mf">0.00001</span><span class="o">,</span>   <span class="c1">// fail if more than 1 in 10,000 errors</span>
  <span class="n">maxSkippedRate</span><span class="k">=</span> <span class="mf">0.01</span>       <span class="c1">// fail if more than 1 in 100 skipped inputs</span>
<span class="o">)</span>
</code></pre></div></div>

<p>Of course, if you are dealing with complex problems, you can also take advantage of custom metrics, additional row-level logging features and advanced state management.</p>

<p>Following the Spark Records data pattern really pays off when jobs fail due to errors and you have to perform root cause analysis. See the <a href="docs.html" title="Spark Records Documentation">documentation</a> for more information.</p>

<h2 id="installation">Installation</h2>

<p>Just add the following to your <code class="language-plaintext highlighter-rouge">libraryDependencies</code> in SBT:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">resolvers</span> <span class="o">+=</span> <span class="nv">Resolver</span><span class="o">.</span><span class="py">bintrayRepo</span><span class="o">(</span><span class="s">"swoop-inc"</span><span class="o">,</span> <span class="s">"maven"</span><span class="o">)</span>

<span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">"com.swoop"</span> <span class="o">%%</span> <span class="s">"spark-records"</span> <span class="o">%</span> <span class="s">"&lt;version&gt;"</span>
</code></pre></div></div>

<p>You can find all released versions <a href="https://github.com/swoop-inc/spark-records/releases">here</a>.</p>

<h2 id="community">Community</h2>

<p>Contributions and feedback of any kind are welcome.</p>

<p>Spark Records is maintained by <a href="https://github.com/ssimeonov" title="Simeon Simeonov">Sim Simeonov</a> and the team at <a href="https://www.swoop.com">Swoop</a>.</p>

<p>Special thanks to <a href="https://github.com/rxin">Reynold Xin</a> and <a href="https://github.com/marmbrus">Michael Armbrust</a> for many interesting conversations about better ways to use Spark.</p>

</div></div></section><section class="technologies"><div class="container"><div class="row"></div></div></section></main><footer id="site-footer"><div class="container"><div class="row"><div class="col-xs-6"><p>Spark Records is designed and developed by <a href="http://www.swoop.com" target="_blank">Swoop</a></p></div><div class="col-xs-6"><p class="text-right"><a href="https://github.com/Shopximity/spark-records"><span class="fa fa-github"></span>View on Github</a></p></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script><script src="/spark-records/highlight/highlight.pack.js"></script><script>hljs.configure({
languages:['scala','java','bash']
});
hljs.initHighlighting();
             </script><script src="/spark-records/js/automenu.js"></script></body></html>